# chunk_and_index.py

import os
import json
import redis
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List

# --------------------------------------------
# 1. C·∫•u h√¨nh k·∫øt n·ªëi Redis Stack v√† embedder SBERT
# --------------------------------------------

# N·∫øu Docker Desktop c·ªßa b·∫°n map port 6380:6379, th√¨ host port ph·∫£i l√† 6380
REDIS_HOST = "localhost"
REDIS_PORT = 6380         # <--- C·ªïng 6380 cho kh·ªõp v·ªõi Docker Desktop
INDEX_NAME = "idx:law"

# Kh·ªüi t·∫°o k·∫øt n·ªëi t·ªõi Redis Stack
# (Redis Stack c√≥ s·∫µn module RediSearch v√† VECTOR HNSW)
r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)

# Kh·ªüi t·∫°o SentenceTransformer
sbert = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")


# --------------------------------------------
# 2. H√†m chia vƒÉn b·∫£n th√†nh c√°c chunk
# --------------------------------------------
def chunk_text(text: str, chunk_size: int = 400, overlap: int = 50) -> List[str]:
    """
    Chia m·ªôt ƒëo·∫°n text d√†i th√†nh c√°c chunk (x·∫•p x·ªâ chunk_size token m·ªói chunk),
    v·ªõi ƒë·ªô overlap gi·ªØa c√°c chunk l√† overlap token.
    Tr·∫£ v·ªÅ list c√°c chunk (m·ªói chunk d∆∞·ªõi d·∫°ng string).
    """
    tokens = text.split()
    chunks = []
    start = 0
    while start < len(tokens):
        end = min(start + chunk_size, len(tokens))
        chunk = tokens[start:end]
        chunks.append(" ".join(chunk))
        if end == len(tokens):
            break
        start += chunk_size - overlap
    return chunks


# --------------------------------------------
# 3. H√†m sinh embedding b·∫±ng SBERT (dim=384)
# --------------------------------------------
def get_embedding(text: str) -> bytes:
    """
    Sinh embedding (float32, dim=384) t·ª´ m·ªôt ƒëo·∫°n text nh·ªù SentenceTransformers,
    r·ªìi convert numpy array nhanh sang bytes ƒë·ªÉ l∆∞u v√†o Redis.
    """
    # encode tr·∫£ v·ªÅ numpy array dtype=float32, shape=(384,)
    emb: np.ndarray = sbert.encode(text, convert_to_numpy=True, normalize_embeddings=True)
    arr = np.array(emb, dtype=np.float32)
    return arr.tobytes()


# --------------------------------------------
# 4. T·∫°o index trong Redis Stack (ch·ªâ ch·∫°y l·∫ßn ƒë·∫ßu)
# --------------------------------------------
def create_redis_index():
    """
    T·∫°o index t√™n idx:law tr√™n Redis Stack ƒë·ªÉ d√πng VECTOR HNSW + text + meta.
    N·∫øu ƒë√£ t·ªìn t·∫°i, b·ªè qua.
    """
    try:
        r.execute_command(
            "FT.CREATE", INDEX_NAME,
            "ON", "HASH",
            "PREFIX", "1", "lawvec:",
            "SCHEMA",
            "vector", "VECTOR", "HNSW", "6",
              "TYPE", "FLOAT32",
              "DIM", "384",
              "DISTANCE_METRIC", "COSINE",
            "text", "TEXT",
            "meta", "TEXT"
        )
        print("‚úÖ ƒê√£ t·∫°o index Redis Stack 'idx:law' th√†nh c√¥ng.")
    except Exception as e:
        # N·∫øu index ƒë√£ t·ªìn t·∫°i, s·∫Ω n√©m exception. B·ªè qua.
        print("‚ö†Ô∏è Index ƒë√£ t·ªìn t·∫°i ho·∫∑c c√≥ l·ªói khi t·∫°o, b·ªè qua.")
        #print("Chi ti·∫øt l·ªói:", e)


# --------------------------------------------
# 5. ƒê·ªçc file .txt, chunk, embed v√† index v√†o Redis
# --------------------------------------------
def index_all_chunks(txt_folder: str):
    """
    Duy·ªát qua t·ª´ng file .txt trong txt_folder, chia chunk, l·∫•y embedding, l∆∞u v√†o Redis.
    - M·ªói chunk l∆∞u d∆∞·ªõi key "lawvec:<s·ªë th·ª© t·ª±>", c√°c field:
        * vector: embedding (bytes float32)
        * text  : n·ªôi dung chunk (string)
        * meta  : metadata d·∫°ng JSON string (v√≠ d·ª• {"source": "filename", "chunk_id": i})
    """
    chunk_counter = 0

    for filename in sorted(os.listdir(txt_folder)):
        if not filename.lower().endswith(".txt"):
            continue

        txt_path = os.path.join(txt_folder, filename)
        with open(txt_path, "r", encoding="utf-8") as f:
            full_text = f.read()

        chunks = chunk_text(full_text, chunk_size=400, overlap=50)
        print(f"‚Üí ƒêang index file '{filename}' v·ªõi {len(chunks)} chunks...")

        for i, chunk in enumerate(chunks):
            meta = {
                "source": filename.replace(".txt", ""),
                "chunk_id": i
                # N·∫øu b·∫°n parse ƒë∆∞·ª£c ƒêi·ªÅu/Kho·∫£n t·ª´ chunk, c√≥ th·ªÉ g√°n meta["dieu"], meta["khoan"] ·ªü ƒë√¢y
            }
            meta_str = json.dumps(meta, ensure_ascii=False)

            emb_bytes = get_embedding(chunk)
            key = f"lawvec:{chunk_counter}"

            r.hset(key, mapping={
                "vector": emb_bytes,
                "text": chunk,
                "meta": meta_str
            })

            chunk_counter += 1
            if chunk_counter % 100 == 0:
                print(f"   ‚Üí ƒê√£ index t·ªïng c·ªông {chunk_counter} chunks...")

    print(f"‚úÖ Ho√†n t·∫•t: ƒë√£ index {chunk_counter} chunks v√†o Redis Stack.")


# --------------------------------------------
# 6. Main: t·∫°o index (n·∫øu c·∫ßn), r·ªìi index to√†n b·ªô chunk
# --------------------------------------------
if __name__ == "__main__":
    # 6.1) T·∫°o index (ch·ªâ ch·∫°y l·∫ßn ƒë·∫ßu, n·∫øu c√≥ r·ªìi th√¨ b·ªè qua)
    create_redis_index()

    # 6.2) Index t·∫•t c·∫£ file .txt trong th∆∞ m·ª•c "plain_texts"
    TXT_FOLDER = "plain_texts"
    if not os.path.isdir(TXT_FOLDER):
        print(f"üö® Th∆∞ m·ª•c '{TXT_FOLDER}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng t·∫°o v√† ƒë·∫∑t file .txt v√†o ƒë√≥.")
    else:
        index_all_chunks(TXT_FOLDER)
